{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd9903b",
   "metadata": {},
   "source": [
    "### Libraries Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944f33e",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fa055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.8: Fast Llama patching. Transformers: 4.52.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a9e030a10c439aa5c00971aec951f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20587a8a0ad64eff9d6c2a8b7c34a3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d850968cea4fda981a6d05c4cb6312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d39cdbadc64953b74c6f74b7f364bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f357d36a805b48ddafc4118892b3df26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdc2b8956724ab988ce1b20775f1869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/3.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# default values chosen by Unsloth for us!\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally! (maximum number of tokens (words/subwords) the model can process at once)\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose  \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f8561",
   "metadata": {},
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "\n",
    "\"*LoRA (Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share.*\" - HuggingFace\n",
    "\n",
    "We can modify the following numbers to increase accuracy, but also counteract over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9e1cb",
   "metadata": {},
   "source": [
    "## Some parameters definition:\n",
    "- **r**: The rank of the finetuning process. A larger number uses more memory and will be slower, but can increase accuracy on harder tasks. We normally suggest numbers like 8 (for fast finetunes), and up to 128. Too large numbers can causing over-fitting, damaging your model's quality.\n",
    "- **target_modules**: Select which parts of the models should be modified by Lora. We select the most important and sensitive modules in transformer models because by updating only these, we can adapt the model to new tasks without changing everything (making it much lighter!).\n",
    "- **lora_alpha**: The scaling factor for finetuning. A larger number will make the finetune learn more about your dataset, but can promote over-fitting. We suggest this to equal to the rank r, or double it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a269ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# default parameters for LoRA (peft=Parameter Efficient Fine-Tuning)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # attention mechanisms modules\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # feed-forward modules\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context and reduce memory usage by an extra 30%\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f804c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# default parameters for LoRA (peft=Parameter Efficient Fine-Tuning)\n",
    "model_ft = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # attention mechanisms modules\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # feed-forward modules\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context and reduce memory usage by an extra 30%\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a2a80",
   "metadata": {},
   "source": [
    "### Test base LLama model with generic question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fibonacci series is a series of numbers in which each number is the sum of the two preceding numbers, starting from 0 and 1. This series is named after the Italian mathematician Leonardo Fibonacci, who introduced it in the 13th century.\n",
      "\n",
      "The Fibonacci series begins like this:\n",
      "\n",
      "0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, and so on.\n",
      "\n",
      "The key property of the Fibonacci series is that each number is the sum of the two preceding numbers. For example:\n",
      "\n",
      "- 0 + 1 = 1\n",
      "- 1 + 1 = 2\n",
      "- 1 + 2 = 3\n",
      "- 2 + 3 = 5\n",
      "- 3 + 5 = 8\n",
      "- 5 + 8 = 13\n",
      "- 8 + 13 = 21\n",
      "- 13 + 21 = 34\n",
      "- And so on.\n",
      "\n",
      "The Fibonacci series appears in many areas of mathematics, science, and nature, such as:\n",
      "\n",
      "1. **Biology**: The arrangement of leaves on a stem, the branching of trees, and the flowering of artichokes follow a Fibonacci pattern.\n",
      "2. **Finance**: The Fibonacci retracement levels are used in technical analysis to predict price movements in financial markets.\n",
      "3. **Geometry**: The Fibonacci spiral is a curve that gets wider by a factor of Ï† (phi) for each quarter turn, where Ï† is the golden ratio.\n",
      "4. **Physics**: The Fibonacci sequence appears in the study of particle physics and the behavior of subatomic particles.\n",
      "\n",
      "The Fibonacci series has many practical applications, including:\n",
      "\n",
      "1. **Design**: The Fibonacci sequence is used in graphic design, architecture, and product design to create aesthetically pleasing and balanced compositions.\n",
      "2. **Finance**: Fibonacci levels are used to predict price movements and set stop-loss levels in trading.\n",
      "3. **Medical imaging**: The Fibonacci sequence is used in medical imaging techniques such as MRI and CT scans to reconstruct images of the body.\n",
      "\n",
      "Overall, the Fibonacci series is a fundamental concept in mathematics and appears in many areas of science and nature, with a wide range of practical applications.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is fibonacci serie?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d690f10",
   "metadata": {},
   "source": [
    "### Test base LLama model with company personal information question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a step-by-step guide on how to request a new company laptop:\n",
      "\n",
      "**Before You Request:**\n",
      "\n",
      "1. **Check company policies**: Review your company's IT department or HR policies to see if there are any specific guidelines or requirements for requesting a new laptop.\n",
      "2. **Assess your needs**: Consider your work requirements and whether a new laptop is necessary. If you're due for an upgrade, use this opportunity to request a new one.\n",
      "\n",
      "**Requesting a New Laptop:**\n",
      "\n",
      "1. **Schedule a meeting with your supervisor or IT representative**: Request a meeting to discuss your laptop needs and request a new one.\n",
      "2. **Prepare your request**: Before the meeting, make a list of your laptop requirements, including:\n",
      "\t* Your current laptop's specifications and condition\n",
      "\t* Your work requirements and how a new laptop will improve your productivity\n",
      "\t* Any specific features or requirements you need (e.g., touchscreen, stylus support, etc.)\n",
      "3. **During the meeting**:\n",
      "\t* Present your request and explain why you need a new laptop\n",
      "\t* Discuss your budget and any costs associated with the new laptop\n",
      "\t* Show any supporting documentation, such as your current laptop's condition or a list of work requirements\n",
      "4. **Follow up**: After the meeting, send a follow-up email to confirm your request and any next steps.\n",
      "\n",
      "**Example Email:**\n",
      "\n",
      "Subject: Request for a New Company Laptop\n",
      "\n",
      "Dear [Supervisor/IT Representative],\n",
      "\n",
      "I hope this email finds you well. I am writing to request a new company laptop as my current one is [insert reason, e.g., outdated, damaged, etc.]. I have attached a list of my current laptop's specifications and a list of my work requirements, which I believe would be improved with a new laptop.\n",
      "\n",
      "I would like to request a laptop with [insert specifications, e.g., touchscreen, stylus support, etc.]. I have done some research and found that [insert brand/model] would be a good option.\n",
      "\n",
      "Could you please let me know if this is feasible and if there are any additional costs or requirements that I need to be aware of?\n",
      "\n",
      "Thank you for considering my request.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "**After the Request:**\n",
      "\n",
      "1. **Wait for a response**: Your supervisor or IT representative will review your request and respond with a decision.\n",
      "2. **Follow up**: If you don't hear back, send a follow-up email to confirm your request and any next steps.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model_ft) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How can I request a new company laptop?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model_ft.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0b248",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad2a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your JSONL file\n",
    "jsonl_file_path = 'company_internal_processes_en.jsonl'\n",
    "\n",
    "data = []\n",
    "with open(jsonl_file_path, 'r') as f:\n",
    "  for line in f:\n",
    "    data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087aaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"What is the policy for posting content on company social media?\",\n          \"Where can I download the company logo in high definition?\",\n          \"What is the procedure for changing project teams?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Marketing approval is required before posting any content.\",\n          \"The logo is available in the 'Media Kit' section of the intranet.\",\n          \"Submit a request to the project manager and send an email to hr@fintaazienda.com.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-48948770-de53-45cd-934d-628774a731e7\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I request a new company laptop?</td>\n",
       "      <td>To request a new company laptop, fill out the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the contact person for IT security trai...</td>\n",
       "      <td>The contact person for IT security training is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where can I find the forms for travel expense ...</td>\n",
       "      <td>The forms can be found in the 'Company Documen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the procedure for reporting a GDPR pol...</td>\n",
       "      <td>Report the violation via email to privacy@fint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who approves leave requests longer than 10 days?</td>\n",
       "      <td>Leave requests longer than 10 days must be app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How do I obtain access to the company VPN from...</td>\n",
       "      <td>Send a request to supporto.it@fintaazienda.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>To whom should monthly expense reports be sent?</td>\n",
       "      <td>Monthly expense reports should be sent to ammi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Where can I find the server maintenance schedule?</td>\n",
       "      <td>The server maintenance schedule is published o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Who is the contact person for company benefits?</td>\n",
       "      <td>For company benefits, you can contact Chiara B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I request access to new management soft...</td>\n",
       "      <td>Request access by filling out the 'Software Ac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48948770-de53-45cd-934d-628774a731e7')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-48948770-de53-45cd-934d-628774a731e7 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-48948770-de53-45cd-934d-628774a731e7');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-1cdc81c4-df9b-4c98-8b41-8ad613e02f18\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1cdc81c4-df9b-4c98-8b41-8ad613e02f18')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-1cdc81c4-df9b-4c98-8b41-8ad613e02f18 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0             How do I request a new company laptop?   \n",
       "1  Who is the contact person for IT security trai...   \n",
       "2  Where can I find the forms for travel expense ...   \n",
       "3  What is the procedure for reporting a GDPR pol...   \n",
       "4   Who approves leave requests longer than 10 days?   \n",
       "5  How do I obtain access to the company VPN from...   \n",
       "6    To whom should monthly expense reports be sent?   \n",
       "7  Where can I find the server maintenance schedule?   \n",
       "8    Who is the contact person for company benefits?   \n",
       "9  How do I request access to new management soft...   \n",
       "\n",
       "                                              answer  \n",
       "0  To request a new company laptop, fill out the ...  \n",
       "1  The contact person for IT security training is...  \n",
       "2  The forms can be found in the 'Company Documen...  \n",
       "3  Report the violation via email to privacy@fint...  \n",
       "4  Leave requests longer than 10 days must be app...  \n",
       "5  Send a request to supporto.it@fintaazienda.com...  \n",
       "6  Monthly expense reports should be sent to ammi...  \n",
       "7  The server maintenance schedule is published o...  \n",
       "8  For company benefits, you can contact Chiara B...  \n",
       "9  Request access by filling out the 'Software Ac...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a77936",
   "metadata": {},
   "source": [
    "### Format it for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "df[\"conversations\"] = df.apply(\n",
    "    lambda x: [\n",
    "        {\"content\": x[\"question\"], \"role\": \"user\"},\n",
    "        {\"content\": x[\"answer\"], \"role\": \"assistant\"}\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "# drop old columns since we now have a single column containing both question and answer formatted as needed\n",
    "dataset = Dataset.from_pandas(df.drop(columns=[\"question\", \"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a3fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'content': 'How do I request a new company laptop?', 'role': 'user'},\n",
       "  {'content': \"To request a new company laptop, fill out the 'IT Asset Request' form on the intranet and send it to asset.it@fintaazienda.com. Approval from your manager is required.\",\n",
       "   'role': 'assistant'}],\n",
       " [{'content': 'Who is the contact person for IT security training?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The contact person for IT security training is Matteo Lorusso from the IT Security department.',\n",
       "   'role': 'assistant'}],\n",
       " [{'content': 'Where can I find the forms for travel expense reporting?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"The forms can be found in the 'Company Documents' section of the intranet, under 'Administration'.\",\n",
       "   'role': 'assistant'}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"conversations\"][:3] # list of lists of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df4887",
   "metadata": {},
   "source": [
    "### Format conversation column adding tags used to train LLama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d9b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2947c62161d3429e8095b6ec34188731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Format the conversations column into a single string using the tokenizer's chat template\n",
    "def format_conversations(example):\n",
    "    # Apply the chat template to the list of messages\n",
    "    # The tokenizer handles the list of dicts and outputs a single formatted string\n",
    "    example[\"formatted_conversations\"] = tokenizer.apply_chat_template(example[\"conversations\"], tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(format_conversations, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d86ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'How do I request a new company laptop?', 'role': 'user'},\n",
       " {'content': \"To request a new company laptop, fill out the 'IT Asset Request' form on the intranet and send it to asset.it@fintaazienda.com. Approval from your manager is required.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"conversations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5612f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do I request a new company laptop?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTo request a new company laptop, fill out the 'IT Asset Request' form on the intranet and send it to asset.it@fintaazienda.com. Approval from your manager is required.<|eot_id|>\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"formatted_conversations\"][0] # string with Llama 3.2 tags separators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3caa8ed",
   "metadata": {},
   "source": [
    "### Train the model!\n",
    "We will use HuggingFace **TRL's SFTTrainer** (Transformer Reinforcement Learning - Supervised Fine Tuning).\n",
    "- TRL is a cutting-edge library designed for post-training foundation models using advanced techniques like Supervised Fine-Tuning (SFT) and others.\n",
    "- Supervised fine-tuning (SFT) is the most common step in post-training foundation models, and also one of the most effective.\n",
    "\n",
    "We do 40 steps to speed things up, but we can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842ca40ccc0f4eca920af404a289ef54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"formatted_conversations\"] (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model_ft,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"formatted_conversations\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 40, # for faster training\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd96b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "6.779 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240b293",
   "metadata": {},
   "source": [
    "### Let's train our model to let it focus on answers only, in order to let it understand the knowledge of our company, disreguarding the types of question that might be asked.\n",
    "\n",
    "\n",
    "The dataset consists of question-answer pairs related to company procedures (e.g., requesting a laptop, reporting GDPR violations).\n",
    "\n",
    "- **Focus on Assistant Responses**: The datasetâ€™s responses hold key procedural knowledge (e.g., forms, emails, approvals). Training only on responses ensures the model learns output style and content without processing redundant questions.\n",
    "\n",
    "- **Computational Efficiency**: Excluding questions reduces memory usage and speeds up fine-tuning, leveraging Unslothâ€™s optimizations for up to 2x faster training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04485af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20b6e2b1e914442970196e4dc5425a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer_on_responses_only = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b18eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 4 | Total steps = 40\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856/3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:11, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.794100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.727200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.664900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.652700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.910700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.950600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.856000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_on_responses_only_stats = trainer_on_responses_only.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049a56a",
   "metadata": {},
   "source": [
    "### Let's test the new fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e24eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill out the 'Laptop Request' form on the intranet and send it to IT@fintaazienda.com.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model_ft) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How can I request a new company laptop?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model_ft.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a491898",
   "metadata": {},
   "source": [
    "### Export the fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.save_pretrained_gguf(\"./model\", tokenizer, quantization_method = \"f16\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "\n",
    "# Save to 8bit Q8_0\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer,)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

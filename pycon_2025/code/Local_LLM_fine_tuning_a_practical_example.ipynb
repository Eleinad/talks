{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "flep__sDDhaM"
   },
   "source": [
    "### Libraries Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "DRskyKHSDw_M"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "NMxvEEHvEFGN"
   },
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348,
     "referenced_widgets": [
      "f5a9e030a10c439aa5c00971aec951f2",
      "40dd80305aeb4b7196f1959bdf1640b3",
      "c9073d08b7e745b1887f2af9c26d3848",
      "837dd29014184b409aee272206bae57b",
      "a539c0a9da964ebcb0b996c2dc3e9264",
      "e5fb409790ce4d9b93db903171eadae0",
      "adc1f0de94e04d10a6f835ca9abd9405",
      "9b47bb0ab5244e328c09a7d686c783d9",
      "312e75d1c7ab4a22aacedd9bd1b18ccb",
      "2abc4e7b460643c7a48b35e3fe609759",
      "d907142695fa4606aaf4f3766c76f8e9",
      "20587a8a0ad64eff9d6c2a8b7c34a3d0",
      "fe4cce73e3b247289d7e0f82db739572",
      "9ec76d0ef2824bf89c1bce25c0471a53",
      "98e2619a672a41f99e0362f364f5de7d",
      "0385884e70534d3e9866c8b55f8cf214",
      "c60ee5a9028b41af83616f8f65696740",
      "cb7e98edcec04b70a45428b79e505c0c",
      "6fc8baaf0cea42fc8fbbbcbcfc2d48da",
      "479006911571422c924cee34ea6d4d47",
      "a3f18c3115044709bde1c1ff4c2b2ce7",
      "a03de54391624435bb4ff801eef2fe93",
      "a5d850968cea4fda981a6d05c4cb6312",
      "3652eecfc73342099930f675baf198a2",
      "0ab8677c5cb44d7ea9e99f3d32e43943",
      "121a45563bdf4affb797671517bc94cf",
      "40112ada49a94036ae476cb7c1a4876f",
      "de91118b865d4cf7ae8df0b0ff9fa65a",
      "b7459d2fb33048818b6be1c759e5230f",
      "6e61c62775584a3abca3ceeb25cdd33a",
      "501dd05892d14137bd14560f0ab18ee2",
      "5d73703997514a9c954431a33ea65b68",
      "2432440f1d10423fac30c7f817e77b50",
      "89d39cdbadc64953b74c6f74b7f364bb",
      "a38c250132f94ebbb126f97a788b6ae3",
      "58ebc123130f4e6d921f1af691fab7e4",
      "3e66aa5bceac42659d2b05ba78c670f9",
      "7ff49f3249dd4115acd21c1370a72aa9",
      "f9adbd37770c42259a357018d0863b36",
      "3e48f79e964743759e312361a0dee347",
      "e8e93dcf26a54bcdae168adc7209f767",
      "613642071fcc4a92aac6b423ab5be065",
      "1eef784a033c487c99f78619637acf49",
      "b933b150226547a2a7bcc13ca2e3dee5",
      "f357d36a805b48ddafc4118892b3df26",
      "e6746e8000594516bee5946365012495",
      "4a64ded78d374b8fb7f7a8ee27418f51",
      "96a2e2f929c243d8a62a6631fd242a8a",
      "59b3c7728be34a8e938145251858eff4",
      "cc15f277a08c434bb56d5e22d6504a7f",
      "661f7ce2e4d74d0dbb0a682911aeb43b",
      "910ac0416d304daa9bdcfe5227c80a8f",
      "4b77684dc70d451b8ba3ad18bbca5aae",
      "5ce35ad69dfe44c4a376cb91e6e60fca",
      "0709e005b6a1429b940484d9b1a282b7",
      "7bdc2b8956724ab988ce1b20775f1869",
      "1c8270c30a3d4015aec020fef5a16a36",
      "fb926b553b2942d38246e867bf16e115",
      "2540e01b29b844bfac9abc18cdde7fe3",
      "0ce8ddd7acaf43c7abfa2a5df1a432af",
      "5d21195ec84049ac9a58593cf3fe424e",
      "9ce9b7e082144653ba18c5cebcd32e9e",
      "866cec9e30c94b6686a746e930346189",
      "b74f61783563427cbff10cab2a01d749",
      "be203ab14d674181ac1a7b8cca1870a6",
      "15be03668c014374a4d1ab8ed00f979c"
     ]
    },
    "id": "gP_2a4HEDyOu",
    "outputId": "f880b905-55b8-472f-b137-f7e5a069388d"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# default values chosen by Unsloth for us!\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally! (maximum number of tokens (words/subwords) the model can process at once)\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose  \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "j3JblReXFcfp"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "\n",
    "\"*LoRA (Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share.*\" - HuggingFace\n",
    "\n",
    "We can modify the following numbers to increase accuracy, but also counteract over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "BYLG_X1PIKWd"
   },
   "source": [
    "## Some parameters definition:\n",
    "- **r**: The rank of the finetuning process. A larger number uses more memory and will be slower, but can increase accuracy on harder tasks. We normally suggest numbers like 8 (for fast finetunes), and up to 128. Too large numbers can causing over-fitting, damaging your model's quality.\n",
    "- **target_modules**: Select which parts of the models should be modified by Lora. We select the most important and sensitive modules in transformer models because by updating only these, we can adapt the model to new tasks without changing everything (making it much lighter!).\n",
    "- **lora_alpha**: The scaling factor for finetuning. A larger number will make the finetune learn more about your dataset, but can promote over-fitting. We suggest this to equal to the rank r, or double it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lj6ks2hqFc7m",
    "outputId": "97e02f6d-0dba-40ad-d601-29c507e9f77a"
   },
   "outputs": [],
   "source": [
    "# default parameters for LoRA (peft=Parameter Efficient Fine-Tuning)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # attention mechanisms modules\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # feed-forward modules\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context and reduce memory usage by an extra 30%\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ASYGGS7XqUci",
    "outputId": "74f0430f-9708-402d-d4ae-8d1847e76b6f"
   },
   "outputs": [],
   "source": [
    "# default parameters for LoRA (peft=Parameter Efficient Fine-Tuning)\n",
    "model_ft = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # attention mechanisms modules\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # feed-forward modules\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context and reduce memory usage by an extra 30%\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "z_KRczltGOSN"
   },
   "source": [
    "### Test base LLama model with generic question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qj59pExGQZi",
    "outputId": "7d68a2ec-2e49-4894-cad0-29f48351967a"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is fibonacci serie?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "R-yxjxoQHeYT"
   },
   "source": [
    "### Test base LLama model with company personal information question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtlEHGKpHiuq",
    "outputId": "d9d78de9-9e88-49a9-bc8b-1c38bfb7193e"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model_ft) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How can I request a new company laptop?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model_ft.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "maPPfL7UOW4z"
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "1dEsbf10OZjH"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your JSONL file\n",
    "jsonl_file_path = 'company_internal_processes_en.jsonl'\n",
    "\n",
    "data = []\n",
    "with open(jsonl_file_path, 'r') as f:\n",
    "  for line in f:\n",
    "    data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "AVP6t4q6Onfq",
    "outputId": "c2fb451f-1af6-4f40-fc13-b3bd4fba638b"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "VBinBPYmQOJ7"
   },
   "source": [
    "### Format it for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "Q9JfMk7KQMtj"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "df[\"conversations\"] = df.apply(\n",
    "    lambda x: [\n",
    "        {\"content\": x[\"question\"], \"role\": \"user\"},\n",
    "        {\"content\": x[\"answer\"], \"role\": \"assistant\"}\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "# drop old columns since we now have a single column containing both question and answer formatted as needed\n",
    "dataset = Dataset.from_pandas(df.drop(columns=[\"question\", \"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bALB6JOIQl4C",
    "outputId": "80e7e3fe-8dbf-4c0b-a3f4-de8b6b3a33ae"
   },
   "outputs": [],
   "source": [
    "dataset[\"conversations\"][:3] # list of lists of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "M9YlyVn2akLs"
   },
   "source": [
    "### Format conversation column adding tags used to train LLama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2947c62161d3429e8095b6ec34188731",
      "a8d9db30aa4d41c2880b87e47a2283ed",
      "7757ba69bf074968825a748e6621f66f",
      "c4f8878174a14fe8be5c410787f3fbb4",
      "e7b7a7417b234313bab554bf3f765f71",
      "e743661527564aa6990f2a245520bcd2",
      "c9abcc0ce1834d0f94db36eaa8b83f26",
      "d5109a3238a942379ac16f3c8e4f8083",
      "43105fb0054e41dbb57d3acf3d77909f",
      "3ed2fa3d81f74acdab6dad2c50e6d45f",
      "2c3b61a534824aafa9c017e1dedf7ce5"
     ]
    },
    "id": "PpX5dO5xWmV2",
    "outputId": "e2db677a-81a2-47e7-cc6e-5347346c0bd2"
   },
   "outputs": [],
   "source": [
    "# Format the conversations column into a single string using the tokenizer's chat template\n",
    "def format_conversations(example):\n",
    "    # Apply the chat template to the list of messages\n",
    "    # The tokenizer handles the list of dicts and outputs a single formatted string\n",
    "    example[\"formatted_conversations\"] = tokenizer.apply_chat_template(example[\"conversations\"], tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(format_conversations, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqDLKanLazR-",
    "outputId": "3b79c15d-1104-4507-9ac9-b45fc196b949"
   },
   "outputs": [],
   "source": [
    "dataset[\"conversations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "e0p_DX64a3iq",
    "outputId": "f433f45f-fd75-4d6c-bde3-5ec79d47cc57"
   },
   "outputs": [],
   "source": [
    "dataset[\"formatted_conversations\"][0] # string with Llama 3.2 tags separators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "g56D3ATNVFW3"
   },
   "source": [
    "### Train the model!\n",
    "We will use HuggingFace **TRL's SFTTrainer** (Transformer Reinforcement Learning - Supervised Fine Tuning).\n",
    "- TRL is a cutting-edge library designed for post-training foundation models using advanced techniques like Supervised Fine-Tuning (SFT) and others.\n",
    "- Supervised fine-tuning (SFT) is the most common step in post-training foundation models, and also one of the most effective.\n",
    "\n",
    "We do 40 steps to speed things up, but we can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "842ca40ccc0f4eca920af404a289ef54",
      "949a3368ccf84d43951f1af2a6dc9f42",
      "7f1232400a0542889f772cb9bf9bee62",
      "2b035d5a2d5341508773d82bc200a3ea",
      "b1c29794bc45427ea7ced95f0df1b572",
      "68d8ec8f55184cf9937b07822caed021",
      "a2603db7e75d476db8ae40bc702ee78f",
      "23e94972ee324326bdae0ea3d06be190",
      "435772dae7e34f1eba7e371d4c3d5058",
      "3fda916ca2b346749ebf4abfeaa7f657",
      "42b33a0e0e2d4c6f811343e8fea050c7"
     ]
    },
    "id": "-DObFYRiUDF0",
    "outputId": "eb8d128c-1344-4ffc-dc0c-a4f4873a1ca0"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model_ft,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"formatted_conversations\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 40, # for faster training\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV18y7SlbGV9",
    "outputId": "fcd0f6a6-401d-4c24-f646-b5d0379a172f"
   },
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "udWTZLLpfP-l"
   },
   "source": [
    "### Let's train our model to let it focus on answers only, in order to let it understand the knowledge of our company, disreguarding the types of question that might be asked.\n",
    "\n",
    "\n",
    "The dataset consists of question-answer pairs related to company procedures (e.g., requesting a laptop, reporting GDPR violations).\n",
    "\n",
    "- **Focus on Assistant Responses**: The dataset’s responses hold key procedural knowledge (e.g., forms, emails, approvals). Training only on responses ensures the model learns output style and content without processing redundant questions.\n",
    "\n",
    "- **Computational Efficiency**: Excluding questions reduces memory usage and speeds up fine-tuning, leveraging Unsloth’s optimizations for up to 2x faster training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e20b6e2b1e914442970196e4dc5425a8",
      "056711161b1445cb97505a0624d4dfaf",
      "7f6579f1218e4ce59665a6b216769894",
      "3e72424b572b4adbab3210a3aa881578",
      "e0c3b473e05542c683d2565dbcce3c3f",
      "cf79052df2024fb4b0744013d8004573",
      "55e4c878ca764af9b41bffc8abc28a99",
      "eee3872655e0411db3bf2d0abab69fc3",
      "07db61b654124ff8a78303c066ccf66e",
      "1b67bc7e003649c1bda8e80b67772e73",
      "e1d0c0559ec44f96a28a73da16aaa691"
     ]
    },
    "id": "VBJjrcbrfw2K",
    "outputId": "b21b31f0-7f8f-4e40-8c95-42b35be6cf42"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer_on_responses_only = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RT5Rav5if58q",
    "outputId": "e18ef6a8-da37-4793-8e40-75929a097a08"
   },
   "outputs": [],
   "source": [
    "trainer_on_responses_only_stats = trainer_on_responses_only.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "P2QCeAKik59r"
   },
   "source": [
    "### Let's test the new fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POOwVu6ggVHK",
    "outputId": "f8e161b5-16c7-4c67-eb59-fd8c7bae8a87"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model_ft) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How can I request a new company laptop?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model_ft.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "PYZ7LBu0AQWA"
   },
   "source": [
    "### Export the fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goPsFDDzl3UE",
    "outputId": "341c5866-02cb-40c6-eaad-f655702101f7"
   },
   "outputs": [],
   "source": [
    "model_ft.save_pretrained_gguf(\"./model\", tokenizer, quantization_method = \"f16\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "\n",
    "# Save to 8bit Q8_0\n",
    "# model.save_pretrained_gguf(\"model\", tokenizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "uiRREe4iphpR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "zgt5oUSZE_wN",
    "outputId": "3719c176-e51b-4716-bcf4-a350725d1e58"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "folder_path = 'model'\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "      if file_path.endswith(\"unsloth.F16.gguf\"):\n",
    "        files.download(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "text_extraction_from_scannedDocs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
